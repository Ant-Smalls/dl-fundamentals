{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29dc571a-e16a-47fa-b456-c4d9367b87a5",
   "metadata": {},
   "source": [
    "# Unit 3, Exercise 2: Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f20142-5f08-4b33-a4e5-ec1315c5a8ff",
   "metadata": {},
   "source": [
    "This exercise is an extension of Exercise 1. Here, the goal is to add code to standardize the features such that they have a mean of 0 and a standard deviation of 1 as discussed in Unit 3.7.\n",
    "\n",
    "Most of the code below is identical to Exercise 1. To avoid not spoil the solution for Exercise 1, the same code parts are missing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5649ce4a-7db3-4716-9fd1-d2db9af5f834",
   "metadata": {},
   "source": [
    "## 1) Installing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea64205-ec41-42db-8167-cd547453354f",
   "metadata": {},
   "source": [
    "You likely already have all libraries installed and don't need to do anything here."
   ]
  },
  {
   "cell_type": "code",
   "id": "ac7e723b-08af-4274-8925-bda4ef60f91a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:17.786705Z",
     "start_time": "2025-02-18T15:47:17.783755Z"
    }
   },
   "source": [
    "# !conda install numpy pandas matplotlib --yes"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "ee966b7b-27cb-4484-b256-8d79f55dc48a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:17.846542Z",
     "start_time": "2025-02-18T15:47:17.844023Z"
    }
   },
   "source": [
    "# !pip install torch"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "1d5d7bb5-c818-4d8c-b6ce-9c6d8fb4ebdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:17.868443Z",
     "start_time": "2025-02-18T15:47:17.865528Z"
    }
   },
   "source": [
    "# !conda install watermark"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "3822a1f2-6b48-4826-9bf4-adbe06c65a37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:17.909623Z",
     "start_time": "2025-02-18T15:47:17.906269Z"
    }
   },
   "source": [
    "#%load_ext watermark\n",
    "#%watermark -v -p numpy,pandas,matplotlib,torch"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "157c6970-2b47-49a1-ba50-59bf738526ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2) Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6638725c-02ee-44db-b661-d882dd191185",
   "metadata": {},
   "source": [
    "We are using the familiar `read_csv` function from pandas to load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "id": "ab70cfad-f8bb-4076-b22e-dffa4f8a48fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:18.627976Z",
     "start_time": "2025-02-18T15:47:17.936479Z"
    }
   },
   "source": [
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "bb8965ae-5222-4541-a7c6-7a9aaa4d1033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:18.660527Z",
     "start_time": "2025-02-18T15:47:18.643506Z"
    }
   },
   "source": [
    "df = pd.read_csv(\"data_banknote_authentication.txt\", header=None)\n",
    "df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         0       1       2        3  4\n",
       "0  3.62160  8.6661 -2.8073 -0.44699  0\n",
       "1  4.54590  8.1674 -2.4586 -1.46210  0\n",
       "2  3.86600 -2.6383  1.9242  0.10645  0\n",
       "3  3.45660  9.5228 -4.0112 -3.59440  0\n",
       "4  0.32924 -4.4552  4.5718 -0.98880  0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "860304f1-1b8c-4993-b547-20e2dcceb03b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:18.765700Z",
     "start_time": "2025-02-18T15:47:18.760927Z"
    }
   },
   "source": [
    "X_features = df[[0, 1, 2, 3]].values\n",
    "y_labels = df[4].values"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "a4d2ebb2-d83f-4729-85ed-9437e105b9b8",
   "metadata": {},
   "source": [
    "Number of examples and features:"
   ]
  },
  {
   "cell_type": "code",
   "id": "6f342b22-0fde-436a-a121-00e9ce627512",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:18.827715Z",
     "start_time": "2025-02-18T15:47:18.823837Z"
    }
   },
   "source": [
    "X_features.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1372, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "7c8e94b9-4847-4833-a7d1-afee3c18991a",
   "metadata": {},
   "source": [
    "It is usually a good idea to look at the label distribution:"
   ]
  },
  {
   "cell_type": "code",
   "id": "1e8247a8-101d-4195-84d3-12b6593c0099",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:18.947652Z",
     "start_time": "2025-02-18T15:47:18.942874Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "np.bincount(y_labels)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([762, 610])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "6f7e7578-c57d-4aae-99fc-77603e202185",
   "metadata": {},
   "source": [
    "## 3) Defining a DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705b52c8-1635-40c8-a6f3-8c4d0d91952e",
   "metadata": {},
   "source": [
    "The `DataLoader` code is the same code code we used in Unit 3.6:"
   ]
  },
  {
   "cell_type": "code",
   "id": "fa4ba92f-f294-4572-8aa2-d2fa50788a9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:20.768175Z",
     "start_time": "2025-02-18T15:47:19.127287Z"
    }
   },
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "\n",
    "        self.features = torch.tensor(X, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.labels[index]        \n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/ant-smalls/Desktop/CPSC352/HW/dl-fundamentals/.venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/ant-smalls/Desktop/CPSC352/HW/dl-fundamentals/.venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/ant-smalls/Desktop/CPSC352/HW/dl-fundamentals/.venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/ant-smalls/Desktop/CPSC352/HW/dl-fundamentals/.venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/ant-smalls/Desktop/CPSC352/HW/dl-fundamentals/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/ant-smalls/Desktop/CPSC352/HW/dl-fundamentals/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/ant-smalls/Desktop/CPSC352/HW/dl-fundamentals/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/ant-smalls/Desktop/CPSC352/HW/dl-fundamentals/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/ant-smalls/Desktop/CPSC352/HW/dl-fundamentals/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/ant-smalls/Desktop/CPSC352/HW/dl-fundamentals/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/ant-smalls/Desktop/CPSC352/HW/dl-fundamentals/.venv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/ant-smalls/Desktop/CPSC352/HW/dl-fundamentals/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/ant-smalls/Desktop/CPSC352/HW/dl-fundamentals/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/ant-smalls/Desktop/CPSC352/HW/dl-fundamentals/.venv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/ant-smalls/Desktop/CPSC352/HW/dl-fundamentals/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/ant-smalls/Desktop/CPSC352/HW/dl-fundamentals/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/ant-smalls/Desktop/CPSC352/HW/dl-fundamentals/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/q6/yfcl_wgs5qg2fgq9b3ld2q140000gn/T/ipykernel_42589/929118859.py\", line 1, in <module>\n",
      "    from torch.utils.data import Dataset, DataLoader\n",
      "  File \"/Users/ant-smalls/Desktop/CPSC352/HW/dl-fundamentals/.venv/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/ant-smalls/Desktop/CPSC352/HW/dl-fundamentals/.venv/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/ant-smalls/Desktop/CPSC352/HW/dl-fundamentals/.venv/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/ant-smalls/Desktop/CPSC352/HW/dl-fundamentals/.venv/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/ant-smalls/Desktop/CPSC352/HW/dl-fundamentals/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/ant-smalls/Desktop/CPSC352/HW/dl-fundamentals/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "e2096f23-539a-40e0-affa-db0ffcd0f371",
   "metadata": {},
   "source": [
    "We will be using 80% of the data for training, 20% of the data for validation. In a real-project, we would also have a separate dataset for the final test set (in this case, we do not have an explicit test set)."
   ]
  },
  {
   "cell_type": "code",
   "id": "f8bbd768-c15c-40f5-8500-83fad5bb1722",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:20.786334Z",
     "start_time": "2025-02-18T15:47:20.782289Z"
    }
   },
   "source": [
    "train_size = int(X_features.shape[0]*0.80)\n",
    "train_size"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1097"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "0b09d3d5-f4f7-47df-8160-8d883c0c5e19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:20.910038Z",
     "start_time": "2025-02-18T15:47:20.903457Z"
    }
   },
   "source": [
    "val_size = X_features.shape[0] - train_size\n",
    "val_size"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "3e21d88e-1408-4457-a7fd-3306a9fac5a6",
   "metadata": {},
   "source": [
    "Using `torch.utils.data.random_split`, we generate the training and validation sets along with the respective data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "id": "22a0e19a-de40-4309-b197-368a781a5633",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:20.987485Z",
     "start_time": "2025-02-18T15:47:20.976602Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "dataset = MyDataset(X_features, y_labels)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_set,\n",
    "    batch_size=10,\n",
    "    shuffle=False,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "28c59d94-f35f-4981-8f33-e7098f13eb49",
   "metadata": {},
   "source": [
    "## 4) Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55784ea-7a16-41bc-a997-a91b010c3b05",
   "metadata": {},
   "source": [
    "There are multiple ways to implement the standardization procedure. For this exercise, we are going to implement a procedure that standardizes the features after we created the data loader.\n",
    "\n",
    "Since this dataset has 4 features, there should be 4 means and 4 standard deviations we compute from the training set. We can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "id": "b6913a35-3222-463b-8c6f-a52a852ec3f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:21.057055Z",
     "start_time": "2025-02-18T15:47:21.006943Z"
    }
   },
   "source": [
    "train_mean = torch.zeros(X_features.shape[1])\n",
    "\n",
    "for x, y in train_loader:\n",
    "    train_mean += x.sum(dim=0)\n",
    "    \n",
    "train_mean /= len(train_set)\n",
    "\n",
    "train_std = torch.zeros(X_features.shape[1])\n",
    "for x, y in train_loader:\n",
    "    train_std += ((x - train_mean)**2).sum(dim=0)\n",
    "\n",
    "train_std = torch.sqrt(train_std / (len(train_set)-1))"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "b04ce773-cf79-4ee6-b9aa-daf2b5614f3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:21.078200Z",
     "start_time": "2025-02-18T15:47:21.072924Z"
    }
   },
   "source": [
    "print(\"Feature means:\", train_mean)\n",
    "print(\"Feature std. devs:\", train_std)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature means: tensor([ 0.3854,  1.8680,  1.4923, -1.1999])\n",
      "Feature std. devs: tensor([2.8575, 5.9216, 4.3869, 2.1041])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "ad6422a6-90a7-4f78-a1b0-8f941c8e75a7",
   "metadata": {},
   "source": [
    "We compute the means and standard deviations by iterating over the training loader. This is an approach that even works for large datasets where the entire dataset doesn't fit into memory. \n",
    "\n",
    "A simpler approach, which only works for smaller datasets that fit into memory, is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "id": "21c5bdab-23c2-47b5-b6bb-65b63ab636d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:21.174483Z",
     "start_time": "2025-02-18T15:47:21.152260Z"
    }
   },
   "source": [
    "all_x = []\n",
    "for x, y in train_loader:\n",
    "    all_x.append(x)\n",
    "    \n",
    "train_std = torch.concat(all_x).std(dim=0)\n",
    "train_mean = torch.concat(all_x).mean(dim=0)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "f4843bdf-d993-48af-82d7-2f072e3553a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:21.201281Z",
     "start_time": "2025-02-18T15:47:21.195375Z"
    }
   },
   "source": [
    "print(\"Feature means:\", train_mean)\n",
    "print(\"Feature std. devs:\", train_std)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature means: tensor([ 0.3854,  1.8680,  1.4923, -1.1999])\n",
      "Feature std. devs: tensor([2.8575, 5.9216, 4.3869, 2.1041])\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "931b7e34-215c-424c-8d38-1dc756782025",
   "metadata": {},
   "source": [
    "<font color='red'>YOUR TASK is now to implement a standardization function based on these training set parameters above:</font>"
   ]
  },
  {
   "cell_type": "code",
   "id": "200d3b72-0ee6-492a-8ef1-72fb856e44b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:21.290185Z",
     "start_time": "2025-02-18T15:47:21.286216Z"
    }
   },
   "source": [
    "def standardize(df, train_mean, train_std): # YOUR CODE\n",
    "    # YOUR CODE\n",
    "    return (df-train_mean) / train_std"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "e3a0b2f5-66f5-45e5-9b0a-f4960fc40388",
   "metadata": {},
   "source": [
    "## 5) Implementing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee409f0-02e0-4591-abf1-5e2c6c41a187",
   "metadata": {},
   "source": [
    "Here, we are resusing the same model code we used in Unit 3.6:"
   ]
  },
  {
   "cell_type": "code",
   "id": "3da86d9a-7cd5-467c-bf65-3388fe272bd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:21.321124Z",
     "start_time": "2025-02-18T15:47:21.316297Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(num_features, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.linear(x)\n",
    "        probas = torch.sigmoid(logits)\n",
    "        return probas"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "c8340676-a3da-49cf-aeae-c0a3329734c5",
   "metadata": {},
   "source": [
    "## 6) The training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1660ca15-18d5-4a55-94f1-e9f543bd8748",
   "metadata": {},
   "source": [
    "In this section, we are using the training loop from Unit 3.6. It's the exact same code except for some small modification: We added the line `if not batch_idx % 20` to only print the lost for every 20th batch (to reduce the number of output lines).\n",
    "\n",
    "<font color='red'>YOUR TASK is to use the standardization code correctly in the for loop. Then, find a good learning rate and epoch number to that you achieve a training and validation performance of at least 98%.</font>"
   ]
  },
  {
   "cell_type": "code",
   "id": "79c712f6-4e2a-43e9-8563-215f88beb4a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:25.622559Z",
     "start_time": "2025-02-18T15:47:21.345542Z"
    }
   },
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = LogisticRegression(num_features=4)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.03) ## possible SOLUTION\n",
    "\n",
    "num_epochs = 75 ## possible SOLUTION\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model = model.train()\n",
    "    for batch_idx, (features, class_labels) in enumerate(train_loader):\n",
    "\n",
    "        features = standardize(features, train_mean, train_std) ## SOLUTION\n",
    "        probas = model(features)\n",
    "        \n",
    "        loss = F.binary_cross_entropy(probas, class_labels.view(probas.shape))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 20: # log every 20th batch\n",
    "            print(f'Epoch: {epoch+1:03d}/{num_epochs:03d}'\n",
    "                   f' | Batch {batch_idx:03d}/{len(train_loader):03d}'\n",
    "                   f' | Loss: {loss:.2f}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/075 | Batch 000/110 | Loss: 0.93\n",
      "Epoch: 001/075 | Batch 020/110 | Loss: 0.74\n",
      "Epoch: 001/075 | Batch 040/110 | Loss: 0.67\n",
      "Epoch: 001/075 | Batch 060/110 | Loss: 0.49\n",
      "Epoch: 001/075 | Batch 080/110 | Loss: 0.43\n",
      "Epoch: 001/075 | Batch 100/110 | Loss: 0.44\n",
      "Epoch: 002/075 | Batch 000/110 | Loss: 0.50\n",
      "Epoch: 002/075 | Batch 020/110 | Loss: 0.38\n",
      "Epoch: 002/075 | Batch 040/110 | Loss: 0.44\n",
      "Epoch: 002/075 | Batch 060/110 | Loss: 0.33\n",
      "Epoch: 002/075 | Batch 080/110 | Loss: 0.31\n",
      "Epoch: 002/075 | Batch 100/110 | Loss: 0.32\n",
      "Epoch: 003/075 | Batch 000/110 | Loss: 0.25\n",
      "Epoch: 003/075 | Batch 020/110 | Loss: 0.35\n",
      "Epoch: 003/075 | Batch 040/110 | Loss: 0.33\n",
      "Epoch: 003/075 | Batch 060/110 | Loss: 0.44\n",
      "Epoch: 003/075 | Batch 080/110 | Loss: 0.26\n",
      "Epoch: 003/075 | Batch 100/110 | Loss: 0.28\n",
      "Epoch: 004/075 | Batch 000/110 | Loss: 0.19\n",
      "Epoch: 004/075 | Batch 020/110 | Loss: 0.15\n",
      "Epoch: 004/075 | Batch 040/110 | Loss: 0.36\n",
      "Epoch: 004/075 | Batch 060/110 | Loss: 0.34\n",
      "Epoch: 004/075 | Batch 080/110 | Loss: 0.16\n",
      "Epoch: 004/075 | Batch 100/110 | Loss: 0.23\n",
      "Epoch: 005/075 | Batch 000/110 | Loss: 0.43\n",
      "Epoch: 005/075 | Batch 020/110 | Loss: 0.24\n",
      "Epoch: 005/075 | Batch 040/110 | Loss: 0.17\n",
      "Epoch: 005/075 | Batch 060/110 | Loss: 0.22\n",
      "Epoch: 005/075 | Batch 080/110 | Loss: 0.26\n",
      "Epoch: 005/075 | Batch 100/110 | Loss: 0.27\n",
      "Epoch: 006/075 | Batch 000/110 | Loss: 0.21\n",
      "Epoch: 006/075 | Batch 020/110 | Loss: 0.18\n",
      "Epoch: 006/075 | Batch 040/110 | Loss: 0.31\n",
      "Epoch: 006/075 | Batch 060/110 | Loss: 0.20\n",
      "Epoch: 006/075 | Batch 080/110 | Loss: 0.15\n",
      "Epoch: 006/075 | Batch 100/110 | Loss: 0.29\n",
      "Epoch: 007/075 | Batch 000/110 | Loss: 0.17\n",
      "Epoch: 007/075 | Batch 020/110 | Loss: 0.07\n",
      "Epoch: 007/075 | Batch 040/110 | Loss: 0.23\n",
      "Epoch: 007/075 | Batch 060/110 | Loss: 0.08\n",
      "Epoch: 007/075 | Batch 080/110 | Loss: 0.28\n",
      "Epoch: 007/075 | Batch 100/110 | Loss: 0.19\n",
      "Epoch: 008/075 | Batch 000/110 | Loss: 0.17\n",
      "Epoch: 008/075 | Batch 020/110 | Loss: 0.19\n",
      "Epoch: 008/075 | Batch 040/110 | Loss: 0.15\n",
      "Epoch: 008/075 | Batch 060/110 | Loss: 0.19\n",
      "Epoch: 008/075 | Batch 080/110 | Loss: 0.14\n",
      "Epoch: 008/075 | Batch 100/110 | Loss: 0.22\n",
      "Epoch: 009/075 | Batch 000/110 | Loss: 0.28\n",
      "Epoch: 009/075 | Batch 020/110 | Loss: 0.25\n",
      "Epoch: 009/075 | Batch 040/110 | Loss: 0.07\n",
      "Epoch: 009/075 | Batch 060/110 | Loss: 0.13\n",
      "Epoch: 009/075 | Batch 080/110 | Loss: 0.14\n",
      "Epoch: 009/075 | Batch 100/110 | Loss: 0.20\n",
      "Epoch: 010/075 | Batch 000/110 | Loss: 0.16\n",
      "Epoch: 010/075 | Batch 020/110 | Loss: 0.13\n",
      "Epoch: 010/075 | Batch 040/110 | Loss: 0.12\n",
      "Epoch: 010/075 | Batch 060/110 | Loss: 0.10\n",
      "Epoch: 010/075 | Batch 080/110 | Loss: 0.16\n",
      "Epoch: 010/075 | Batch 100/110 | Loss: 0.05\n",
      "Epoch: 011/075 | Batch 000/110 | Loss: 0.10\n",
      "Epoch: 011/075 | Batch 020/110 | Loss: 0.08\n",
      "Epoch: 011/075 | Batch 040/110 | Loss: 0.26\n",
      "Epoch: 011/075 | Batch 060/110 | Loss: 0.13\n",
      "Epoch: 011/075 | Batch 080/110 | Loss: 0.17\n",
      "Epoch: 011/075 | Batch 100/110 | Loss: 0.10\n",
      "Epoch: 012/075 | Batch 000/110 | Loss: 0.19\n",
      "Epoch: 012/075 | Batch 020/110 | Loss: 0.08\n",
      "Epoch: 012/075 | Batch 040/110 | Loss: 0.12\n",
      "Epoch: 012/075 | Batch 060/110 | Loss: 0.09\n",
      "Epoch: 012/075 | Batch 080/110 | Loss: 0.15\n",
      "Epoch: 012/075 | Batch 100/110 | Loss: 0.08\n",
      "Epoch: 013/075 | Batch 000/110 | Loss: 0.11\n",
      "Epoch: 013/075 | Batch 020/110 | Loss: 0.10\n",
      "Epoch: 013/075 | Batch 040/110 | Loss: 0.15\n",
      "Epoch: 013/075 | Batch 060/110 | Loss: 0.14\n",
      "Epoch: 013/075 | Batch 080/110 | Loss: 0.23\n",
      "Epoch: 013/075 | Batch 100/110 | Loss: 0.32\n",
      "Epoch: 014/075 | Batch 000/110 | Loss: 0.12\n",
      "Epoch: 014/075 | Batch 020/110 | Loss: 0.11\n",
      "Epoch: 014/075 | Batch 040/110 | Loss: 0.07\n",
      "Epoch: 014/075 | Batch 060/110 | Loss: 0.17\n",
      "Epoch: 014/075 | Batch 080/110 | Loss: 0.08\n",
      "Epoch: 014/075 | Batch 100/110 | Loss: 0.11\n",
      "Epoch: 015/075 | Batch 000/110 | Loss: 0.17\n",
      "Epoch: 015/075 | Batch 020/110 | Loss: 0.05\n",
      "Epoch: 015/075 | Batch 040/110 | Loss: 0.06\n",
      "Epoch: 015/075 | Batch 060/110 | Loss: 0.04\n",
      "Epoch: 015/075 | Batch 080/110 | Loss: 0.10\n",
      "Epoch: 015/075 | Batch 100/110 | Loss: 0.27\n",
      "Epoch: 016/075 | Batch 000/110 | Loss: 0.06\n",
      "Epoch: 016/075 | Batch 020/110 | Loss: 0.07\n",
      "Epoch: 016/075 | Batch 040/110 | Loss: 0.10\n",
      "Epoch: 016/075 | Batch 060/110 | Loss: 0.14\n",
      "Epoch: 016/075 | Batch 080/110 | Loss: 0.05\n",
      "Epoch: 016/075 | Batch 100/110 | Loss: 0.12\n",
      "Epoch: 017/075 | Batch 000/110 | Loss: 0.05\n",
      "Epoch: 017/075 | Batch 020/110 | Loss: 0.22\n",
      "Epoch: 017/075 | Batch 040/110 | Loss: 0.04\n",
      "Epoch: 017/075 | Batch 060/110 | Loss: 0.13\n",
      "Epoch: 017/075 | Batch 080/110 | Loss: 0.08\n",
      "Epoch: 017/075 | Batch 100/110 | Loss: 0.14\n",
      "Epoch: 018/075 | Batch 000/110 | Loss: 0.09\n",
      "Epoch: 018/075 | Batch 020/110 | Loss: 0.08\n",
      "Epoch: 018/075 | Batch 040/110 | Loss: 0.06\n",
      "Epoch: 018/075 | Batch 060/110 | Loss: 0.13\n",
      "Epoch: 018/075 | Batch 080/110 | Loss: 0.11\n",
      "Epoch: 018/075 | Batch 100/110 | Loss: 0.05\n",
      "Epoch: 019/075 | Batch 000/110 | Loss: 0.07\n",
      "Epoch: 019/075 | Batch 020/110 | Loss: 0.15\n",
      "Epoch: 019/075 | Batch 040/110 | Loss: 0.06\n",
      "Epoch: 019/075 | Batch 060/110 | Loss: 0.10\n",
      "Epoch: 019/075 | Batch 080/110 | Loss: 0.05\n",
      "Epoch: 019/075 | Batch 100/110 | Loss: 0.17\n",
      "Epoch: 020/075 | Batch 000/110 | Loss: 0.11\n",
      "Epoch: 020/075 | Batch 020/110 | Loss: 0.09\n",
      "Epoch: 020/075 | Batch 040/110 | Loss: 0.09\n",
      "Epoch: 020/075 | Batch 060/110 | Loss: 0.07\n",
      "Epoch: 020/075 | Batch 080/110 | Loss: 0.06\n",
      "Epoch: 020/075 | Batch 100/110 | Loss: 0.11\n",
      "Epoch: 021/075 | Batch 000/110 | Loss: 0.18\n",
      "Epoch: 021/075 | Batch 020/110 | Loss: 0.05\n",
      "Epoch: 021/075 | Batch 040/110 | Loss: 0.16\n",
      "Epoch: 021/075 | Batch 060/110 | Loss: 0.06\n",
      "Epoch: 021/075 | Batch 080/110 | Loss: 0.06\n",
      "Epoch: 021/075 | Batch 100/110 | Loss: 0.14\n",
      "Epoch: 022/075 | Batch 000/110 | Loss: 0.08\n",
      "Epoch: 022/075 | Batch 020/110 | Loss: 0.17\n",
      "Epoch: 022/075 | Batch 040/110 | Loss: 0.05\n",
      "Epoch: 022/075 | Batch 060/110 | Loss: 0.09\n",
      "Epoch: 022/075 | Batch 080/110 | Loss: 0.16\n",
      "Epoch: 022/075 | Batch 100/110 | Loss: 0.06\n",
      "Epoch: 023/075 | Batch 000/110 | Loss: 0.04\n",
      "Epoch: 023/075 | Batch 020/110 | Loss: 0.15\n",
      "Epoch: 023/075 | Batch 040/110 | Loss: 0.06\n",
      "Epoch: 023/075 | Batch 060/110 | Loss: 0.08\n",
      "Epoch: 023/075 | Batch 080/110 | Loss: 0.07\n",
      "Epoch: 023/075 | Batch 100/110 | Loss: 0.10\n",
      "Epoch: 024/075 | Batch 000/110 | Loss: 0.11\n",
      "Epoch: 024/075 | Batch 020/110 | Loss: 0.20\n",
      "Epoch: 024/075 | Batch 040/110 | Loss: 0.04\n",
      "Epoch: 024/075 | Batch 060/110 | Loss: 0.08\n",
      "Epoch: 024/075 | Batch 080/110 | Loss: 0.12\n",
      "Epoch: 024/075 | Batch 100/110 | Loss: 0.08\n",
      "Epoch: 025/075 | Batch 000/110 | Loss: 0.04\n",
      "Epoch: 025/075 | Batch 020/110 | Loss: 0.04\n",
      "Epoch: 025/075 | Batch 040/110 | Loss: 0.08\n",
      "Epoch: 025/075 | Batch 060/110 | Loss: 0.09\n",
      "Epoch: 025/075 | Batch 080/110 | Loss: 0.05\n",
      "Epoch: 025/075 | Batch 100/110 | Loss: 0.06\n",
      "Epoch: 026/075 | Batch 000/110 | Loss: 0.13\n",
      "Epoch: 026/075 | Batch 020/110 | Loss: 0.08\n",
      "Epoch: 026/075 | Batch 040/110 | Loss: 0.13\n",
      "Epoch: 026/075 | Batch 060/110 | Loss: 0.10\n",
      "Epoch: 026/075 | Batch 080/110 | Loss: 0.11\n",
      "Epoch: 026/075 | Batch 100/110 | Loss: 0.08\n",
      "Epoch: 027/075 | Batch 000/110 | Loss: 0.05\n",
      "Epoch: 027/075 | Batch 020/110 | Loss: 0.03\n",
      "Epoch: 027/075 | Batch 040/110 | Loss: 0.05\n",
      "Epoch: 027/075 | Batch 060/110 | Loss: 0.15\n",
      "Epoch: 027/075 | Batch 080/110 | Loss: 0.04\n",
      "Epoch: 027/075 | Batch 100/110 | Loss: 0.03\n",
      "Epoch: 028/075 | Batch 000/110 | Loss: 0.20\n",
      "Epoch: 028/075 | Batch 020/110 | Loss: 0.06\n",
      "Epoch: 028/075 | Batch 040/110 | Loss: 0.14\n",
      "Epoch: 028/075 | Batch 060/110 | Loss: 0.08\n",
      "Epoch: 028/075 | Batch 080/110 | Loss: 0.06\n",
      "Epoch: 028/075 | Batch 100/110 | Loss: 0.14\n",
      "Epoch: 029/075 | Batch 000/110 | Loss: 0.16\n",
      "Epoch: 029/075 | Batch 020/110 | Loss: 0.07\n",
      "Epoch: 029/075 | Batch 040/110 | Loss: 0.11\n",
      "Epoch: 029/075 | Batch 060/110 | Loss: 0.05\n",
      "Epoch: 029/075 | Batch 080/110 | Loss: 0.13\n",
      "Epoch: 029/075 | Batch 100/110 | Loss: 0.03\n",
      "Epoch: 030/075 | Batch 000/110 | Loss: 0.20\n",
      "Epoch: 030/075 | Batch 020/110 | Loss: 0.03\n",
      "Epoch: 030/075 | Batch 040/110 | Loss: 0.05\n",
      "Epoch: 030/075 | Batch 060/110 | Loss: 0.13\n",
      "Epoch: 030/075 | Batch 080/110 | Loss: 0.10\n",
      "Epoch: 030/075 | Batch 100/110 | Loss: 0.04\n",
      "Epoch: 031/075 | Batch 000/110 | Loss: 0.09\n",
      "Epoch: 031/075 | Batch 020/110 | Loss: 0.07\n",
      "Epoch: 031/075 | Batch 040/110 | Loss: 0.11\n",
      "Epoch: 031/075 | Batch 060/110 | Loss: 0.06\n",
      "Epoch: 031/075 | Batch 080/110 | Loss: 0.08\n",
      "Epoch: 031/075 | Batch 100/110 | Loss: 0.09\n",
      "Epoch: 032/075 | Batch 000/110 | Loss: 0.01\n",
      "Epoch: 032/075 | Batch 020/110 | Loss: 0.04\n",
      "Epoch: 032/075 | Batch 040/110 | Loss: 0.15\n",
      "Epoch: 032/075 | Batch 060/110 | Loss: 0.03\n",
      "Epoch: 032/075 | Batch 080/110 | Loss: 0.05\n",
      "Epoch: 032/075 | Batch 100/110 | Loss: 0.12\n",
      "Epoch: 033/075 | Batch 000/110 | Loss: 0.05\n",
      "Epoch: 033/075 | Batch 020/110 | Loss: 0.12\n",
      "Epoch: 033/075 | Batch 040/110 | Loss: 0.16\n",
      "Epoch: 033/075 | Batch 060/110 | Loss: 0.03\n",
      "Epoch: 033/075 | Batch 080/110 | Loss: 0.09\n",
      "Epoch: 033/075 | Batch 100/110 | Loss: 0.10\n",
      "Epoch: 034/075 | Batch 000/110 | Loss: 0.07\n",
      "Epoch: 034/075 | Batch 020/110 | Loss: 0.05\n",
      "Epoch: 034/075 | Batch 040/110 | Loss: 0.07\n",
      "Epoch: 034/075 | Batch 060/110 | Loss: 0.07\n",
      "Epoch: 034/075 | Batch 080/110 | Loss: 0.05\n",
      "Epoch: 034/075 | Batch 100/110 | Loss: 0.03\n",
      "Epoch: 035/075 | Batch 000/110 | Loss: 0.05\n",
      "Epoch: 035/075 | Batch 020/110 | Loss: 0.06\n",
      "Epoch: 035/075 | Batch 040/110 | Loss: 0.02\n",
      "Epoch: 035/075 | Batch 060/110 | Loss: 0.07\n",
      "Epoch: 035/075 | Batch 080/110 | Loss: 0.06\n",
      "Epoch: 035/075 | Batch 100/110 | Loss: 0.04\n",
      "Epoch: 036/075 | Batch 000/110 | Loss: 0.07\n",
      "Epoch: 036/075 | Batch 020/110 | Loss: 0.04\n",
      "Epoch: 036/075 | Batch 040/110 | Loss: 0.05\n",
      "Epoch: 036/075 | Batch 060/110 | Loss: 0.03\n",
      "Epoch: 036/075 | Batch 080/110 | Loss: 0.12\n",
      "Epoch: 036/075 | Batch 100/110 | Loss: 0.13\n",
      "Epoch: 037/075 | Batch 000/110 | Loss: 0.09\n",
      "Epoch: 037/075 | Batch 020/110 | Loss: 0.03\n",
      "Epoch: 037/075 | Batch 040/110 | Loss: 0.05\n",
      "Epoch: 037/075 | Batch 060/110 | Loss: 0.03\n",
      "Epoch: 037/075 | Batch 080/110 | Loss: 0.02\n",
      "Epoch: 037/075 | Batch 100/110 | Loss: 0.27\n",
      "Epoch: 038/075 | Batch 000/110 | Loss: 0.09\n",
      "Epoch: 038/075 | Batch 020/110 | Loss: 0.02\n",
      "Epoch: 038/075 | Batch 040/110 | Loss: 0.07\n",
      "Epoch: 038/075 | Batch 060/110 | Loss: 0.06\n",
      "Epoch: 038/075 | Batch 080/110 | Loss: 0.07\n",
      "Epoch: 038/075 | Batch 100/110 | Loss: 0.13\n",
      "Epoch: 039/075 | Batch 000/110 | Loss: 0.16\n",
      "Epoch: 039/075 | Batch 020/110 | Loss: 0.04\n",
      "Epoch: 039/075 | Batch 040/110 | Loss: 0.26\n",
      "Epoch: 039/075 | Batch 060/110 | Loss: 0.02\n",
      "Epoch: 039/075 | Batch 080/110 | Loss: 0.07\n",
      "Epoch: 039/075 | Batch 100/110 | Loss: 0.09\n",
      "Epoch: 040/075 | Batch 000/110 | Loss: 0.21\n",
      "Epoch: 040/075 | Batch 020/110 | Loss: 0.05\n",
      "Epoch: 040/075 | Batch 040/110 | Loss: 0.15\n",
      "Epoch: 040/075 | Batch 060/110 | Loss: 0.05\n",
      "Epoch: 040/075 | Batch 080/110 | Loss: 0.08\n",
      "Epoch: 040/075 | Batch 100/110 | Loss: 0.06\n",
      "Epoch: 041/075 | Batch 000/110 | Loss: 0.03\n",
      "Epoch: 041/075 | Batch 020/110 | Loss: 0.17\n",
      "Epoch: 041/075 | Batch 040/110 | Loss: 0.13\n",
      "Epoch: 041/075 | Batch 060/110 | Loss: 0.09\n",
      "Epoch: 041/075 | Batch 080/110 | Loss: 0.31\n",
      "Epoch: 041/075 | Batch 100/110 | Loss: 0.06\n",
      "Epoch: 042/075 | Batch 000/110 | Loss: 0.10\n",
      "Epoch: 042/075 | Batch 020/110 | Loss: 0.05\n",
      "Epoch: 042/075 | Batch 040/110 | Loss: 0.02\n",
      "Epoch: 042/075 | Batch 060/110 | Loss: 0.03\n",
      "Epoch: 042/075 | Batch 080/110 | Loss: 0.17\n",
      "Epoch: 042/075 | Batch 100/110 | Loss: 0.07\n",
      "Epoch: 043/075 | Batch 000/110 | Loss: 0.02\n",
      "Epoch: 043/075 | Batch 020/110 | Loss: 0.05\n",
      "Epoch: 043/075 | Batch 040/110 | Loss: 0.15\n",
      "Epoch: 043/075 | Batch 060/110 | Loss: 0.03\n",
      "Epoch: 043/075 | Batch 080/110 | Loss: 0.08\n",
      "Epoch: 043/075 | Batch 100/110 | Loss: 0.03\n",
      "Epoch: 044/075 | Batch 000/110 | Loss: 0.05\n",
      "Epoch: 044/075 | Batch 020/110 | Loss: 0.08\n",
      "Epoch: 044/075 | Batch 040/110 | Loss: 0.03\n",
      "Epoch: 044/075 | Batch 060/110 | Loss: 0.07\n",
      "Epoch: 044/075 | Batch 080/110 | Loss: 0.13\n",
      "Epoch: 044/075 | Batch 100/110 | Loss: 0.02\n",
      "Epoch: 045/075 | Batch 000/110 | Loss: 0.05\n",
      "Epoch: 045/075 | Batch 020/110 | Loss: 0.13\n",
      "Epoch: 045/075 | Batch 040/110 | Loss: 0.10\n",
      "Epoch: 045/075 | Batch 060/110 | Loss: 0.06\n",
      "Epoch: 045/075 | Batch 080/110 | Loss: 0.15\n",
      "Epoch: 045/075 | Batch 100/110 | Loss: 0.07\n",
      "Epoch: 046/075 | Batch 000/110 | Loss: 0.03\n",
      "Epoch: 046/075 | Batch 020/110 | Loss: 0.07\n",
      "Epoch: 046/075 | Batch 040/110 | Loss: 0.07\n",
      "Epoch: 046/075 | Batch 060/110 | Loss: 0.03\n",
      "Epoch: 046/075 | Batch 080/110 | Loss: 0.04\n",
      "Epoch: 046/075 | Batch 100/110 | Loss: 0.03\n",
      "Epoch: 047/075 | Batch 000/110 | Loss: 0.04\n",
      "Epoch: 047/075 | Batch 020/110 | Loss: 0.06\n",
      "Epoch: 047/075 | Batch 040/110 | Loss: 0.19\n",
      "Epoch: 047/075 | Batch 060/110 | Loss: 0.02\n",
      "Epoch: 047/075 | Batch 080/110 | Loss: 0.04\n",
      "Epoch: 047/075 | Batch 100/110 | Loss: 0.11\n",
      "Epoch: 048/075 | Batch 000/110 | Loss: 0.03\n",
      "Epoch: 048/075 | Batch 020/110 | Loss: 0.05\n",
      "Epoch: 048/075 | Batch 040/110 | Loss: 0.18\n",
      "Epoch: 048/075 | Batch 060/110 | Loss: 0.06\n",
      "Epoch: 048/075 | Batch 080/110 | Loss: 0.06\n",
      "Epoch: 048/075 | Batch 100/110 | Loss: 0.08\n",
      "Epoch: 049/075 | Batch 000/110 | Loss: 0.04\n",
      "Epoch: 049/075 | Batch 020/110 | Loss: 0.04\n",
      "Epoch: 049/075 | Batch 040/110 | Loss: 0.13\n",
      "Epoch: 049/075 | Batch 060/110 | Loss: 0.07\n",
      "Epoch: 049/075 | Batch 080/110 | Loss: 0.01\n",
      "Epoch: 049/075 | Batch 100/110 | Loss: 0.02\n",
      "Epoch: 050/075 | Batch 000/110 | Loss: 0.13\n",
      "Epoch: 050/075 | Batch 020/110 | Loss: 0.05\n",
      "Epoch: 050/075 | Batch 040/110 | Loss: 0.13\n",
      "Epoch: 050/075 | Batch 060/110 | Loss: 0.04\n",
      "Epoch: 050/075 | Batch 080/110 | Loss: 0.05\n",
      "Epoch: 050/075 | Batch 100/110 | Loss: 0.03\n",
      "Epoch: 051/075 | Batch 000/110 | Loss: 0.03\n",
      "Epoch: 051/075 | Batch 020/110 | Loss: 0.01\n",
      "Epoch: 051/075 | Batch 040/110 | Loss: 0.12\n",
      "Epoch: 051/075 | Batch 060/110 | Loss: 0.03\n",
      "Epoch: 051/075 | Batch 080/110 | Loss: 0.02\n",
      "Epoch: 051/075 | Batch 100/110 | Loss: 0.21\n",
      "Epoch: 052/075 | Batch 000/110 | Loss: 0.09\n",
      "Epoch: 052/075 | Batch 020/110 | Loss: 0.13\n",
      "Epoch: 052/075 | Batch 040/110 | Loss: 0.02\n",
      "Epoch: 052/075 | Batch 060/110 | Loss: 0.02\n",
      "Epoch: 052/075 | Batch 080/110 | Loss: 0.25\n",
      "Epoch: 052/075 | Batch 100/110 | Loss: 0.05\n",
      "Epoch: 053/075 | Batch 000/110 | Loss: 0.04\n",
      "Epoch: 053/075 | Batch 020/110 | Loss: 0.15\n",
      "Epoch: 053/075 | Batch 040/110 | Loss: 0.05\n",
      "Epoch: 053/075 | Batch 060/110 | Loss: 0.05\n",
      "Epoch: 053/075 | Batch 080/110 | Loss: 0.01\n",
      "Epoch: 053/075 | Batch 100/110 | Loss: 0.04\n",
      "Epoch: 054/075 | Batch 000/110 | Loss: 0.05\n",
      "Epoch: 054/075 | Batch 020/110 | Loss: 0.03\n",
      "Epoch: 054/075 | Batch 040/110 | Loss: 0.14\n",
      "Epoch: 054/075 | Batch 060/110 | Loss: 0.07\n",
      "Epoch: 054/075 | Batch 080/110 | Loss: 0.05\n",
      "Epoch: 054/075 | Batch 100/110 | Loss: 0.05\n",
      "Epoch: 055/075 | Batch 000/110 | Loss: 0.01\n",
      "Epoch: 055/075 | Batch 020/110 | Loss: 0.04\n",
      "Epoch: 055/075 | Batch 040/110 | Loss: 0.02\n",
      "Epoch: 055/075 | Batch 060/110 | Loss: 0.04\n",
      "Epoch: 055/075 | Batch 080/110 | Loss: 0.07\n",
      "Epoch: 055/075 | Batch 100/110 | Loss: 0.15\n",
      "Epoch: 056/075 | Batch 000/110 | Loss: 0.02\n",
      "Epoch: 056/075 | Batch 020/110 | Loss: 0.15\n",
      "Epoch: 056/075 | Batch 040/110 | Loss: 0.06\n",
      "Epoch: 056/075 | Batch 060/110 | Loss: 0.16\n",
      "Epoch: 056/075 | Batch 080/110 | Loss: 0.02\n",
      "Epoch: 056/075 | Batch 100/110 | Loss: 0.01\n",
      "Epoch: 057/075 | Batch 000/110 | Loss: 0.02\n",
      "Epoch: 057/075 | Batch 020/110 | Loss: 0.05\n",
      "Epoch: 057/075 | Batch 040/110 | Loss: 0.20\n",
      "Epoch: 057/075 | Batch 060/110 | Loss: 0.14\n",
      "Epoch: 057/075 | Batch 080/110 | Loss: 0.13\n",
      "Epoch: 057/075 | Batch 100/110 | Loss: 0.03\n",
      "Epoch: 058/075 | Batch 000/110 | Loss: 0.09\n",
      "Epoch: 058/075 | Batch 020/110 | Loss: 0.05\n",
      "Epoch: 058/075 | Batch 040/110 | Loss: 0.04\n",
      "Epoch: 058/075 | Batch 060/110 | Loss: 0.05\n",
      "Epoch: 058/075 | Batch 080/110 | Loss: 0.02\n",
      "Epoch: 058/075 | Batch 100/110 | Loss: 0.03\n",
      "Epoch: 059/075 | Batch 000/110 | Loss: 0.02\n",
      "Epoch: 059/075 | Batch 020/110 | Loss: 0.04\n",
      "Epoch: 059/075 | Batch 040/110 | Loss: 0.04\n",
      "Epoch: 059/075 | Batch 060/110 | Loss: 0.13\n",
      "Epoch: 059/075 | Batch 080/110 | Loss: 0.04\n",
      "Epoch: 059/075 | Batch 100/110 | Loss: 0.03\n",
      "Epoch: 060/075 | Batch 000/110 | Loss: 0.06\n",
      "Epoch: 060/075 | Batch 020/110 | Loss: 0.27\n",
      "Epoch: 060/075 | Batch 040/110 | Loss: 0.12\n",
      "Epoch: 060/075 | Batch 060/110 | Loss: 0.08\n",
      "Epoch: 060/075 | Batch 080/110 | Loss: 0.05\n",
      "Epoch: 060/075 | Batch 100/110 | Loss: 0.06\n",
      "Epoch: 061/075 | Batch 000/110 | Loss: 0.06\n",
      "Epoch: 061/075 | Batch 020/110 | Loss: 0.02\n",
      "Epoch: 061/075 | Batch 040/110 | Loss: 0.11\n",
      "Epoch: 061/075 | Batch 060/110 | Loss: 0.13\n",
      "Epoch: 061/075 | Batch 080/110 | Loss: 0.06\n",
      "Epoch: 061/075 | Batch 100/110 | Loss: 0.04\n",
      "Epoch: 062/075 | Batch 000/110 | Loss: 0.04\n",
      "Epoch: 062/075 | Batch 020/110 | Loss: 0.04\n",
      "Epoch: 062/075 | Batch 040/110 | Loss: 0.16\n",
      "Epoch: 062/075 | Batch 060/110 | Loss: 0.02\n",
      "Epoch: 062/075 | Batch 080/110 | Loss: 0.02\n",
      "Epoch: 062/075 | Batch 100/110 | Loss: 0.12\n",
      "Epoch: 063/075 | Batch 000/110 | Loss: 0.01\n",
      "Epoch: 063/075 | Batch 020/110 | Loss: 0.09\n",
      "Epoch: 063/075 | Batch 040/110 | Loss: 0.02\n",
      "Epoch: 063/075 | Batch 060/110 | Loss: 0.25\n",
      "Epoch: 063/075 | Batch 080/110 | Loss: 0.06\n",
      "Epoch: 063/075 | Batch 100/110 | Loss: 0.02\n",
      "Epoch: 064/075 | Batch 000/110 | Loss: 0.05\n",
      "Epoch: 064/075 | Batch 020/110 | Loss: 0.04\n",
      "Epoch: 064/075 | Batch 040/110 | Loss: 0.05\n",
      "Epoch: 064/075 | Batch 060/110 | Loss: 0.04\n",
      "Epoch: 064/075 | Batch 080/110 | Loss: 0.01\n",
      "Epoch: 064/075 | Batch 100/110 | Loss: 0.15\n",
      "Epoch: 065/075 | Batch 000/110 | Loss: 0.06\n",
      "Epoch: 065/075 | Batch 020/110 | Loss: 0.02\n",
      "Epoch: 065/075 | Batch 040/110 | Loss: 0.02\n",
      "Epoch: 065/075 | Batch 060/110 | Loss: 0.04\n",
      "Epoch: 065/075 | Batch 080/110 | Loss: 0.02\n",
      "Epoch: 065/075 | Batch 100/110 | Loss: 0.04\n",
      "Epoch: 066/075 | Batch 000/110 | Loss: 0.13\n",
      "Epoch: 066/075 | Batch 020/110 | Loss: 0.05\n",
      "Epoch: 066/075 | Batch 040/110 | Loss: 0.13\n",
      "Epoch: 066/075 | Batch 060/110 | Loss: 0.06\n",
      "Epoch: 066/075 | Batch 080/110 | Loss: 0.03\n",
      "Epoch: 066/075 | Batch 100/110 | Loss: 0.05\n",
      "Epoch: 067/075 | Batch 000/110 | Loss: 0.04\n",
      "Epoch: 067/075 | Batch 020/110 | Loss: 0.02\n",
      "Epoch: 067/075 | Batch 040/110 | Loss: 0.10\n",
      "Epoch: 067/075 | Batch 060/110 | Loss: 0.02\n",
      "Epoch: 067/075 | Batch 080/110 | Loss: 0.10\n",
      "Epoch: 067/075 | Batch 100/110 | Loss: 0.04\n",
      "Epoch: 068/075 | Batch 000/110 | Loss: 0.16\n",
      "Epoch: 068/075 | Batch 020/110 | Loss: 0.18\n",
      "Epoch: 068/075 | Batch 040/110 | Loss: 0.22\n",
      "Epoch: 068/075 | Batch 060/110 | Loss: 0.13\n",
      "Epoch: 068/075 | Batch 080/110 | Loss: 0.05\n",
      "Epoch: 068/075 | Batch 100/110 | Loss: 0.06\n",
      "Epoch: 069/075 | Batch 000/110 | Loss: 0.03\n",
      "Epoch: 069/075 | Batch 020/110 | Loss: 0.04\n",
      "Epoch: 069/075 | Batch 040/110 | Loss: 0.03\n",
      "Epoch: 069/075 | Batch 060/110 | Loss: 0.07\n",
      "Epoch: 069/075 | Batch 080/110 | Loss: 0.02\n",
      "Epoch: 069/075 | Batch 100/110 | Loss: 0.03\n",
      "Epoch: 070/075 | Batch 000/110 | Loss: 0.03\n",
      "Epoch: 070/075 | Batch 020/110 | Loss: 0.01\n",
      "Epoch: 070/075 | Batch 040/110 | Loss: 0.02\n",
      "Epoch: 070/075 | Batch 060/110 | Loss: 0.02\n",
      "Epoch: 070/075 | Batch 080/110 | Loss: 0.04\n",
      "Epoch: 070/075 | Batch 100/110 | Loss: 0.12\n",
      "Epoch: 071/075 | Batch 000/110 | Loss: 0.02\n",
      "Epoch: 071/075 | Batch 020/110 | Loss: 0.08\n",
      "Epoch: 071/075 | Batch 040/110 | Loss: 0.12\n",
      "Epoch: 071/075 | Batch 060/110 | Loss: 0.01\n",
      "Epoch: 071/075 | Batch 080/110 | Loss: 0.03\n",
      "Epoch: 071/075 | Batch 100/110 | Loss: 0.04\n",
      "Epoch: 072/075 | Batch 000/110 | Loss: 0.06\n",
      "Epoch: 072/075 | Batch 020/110 | Loss: 0.06\n",
      "Epoch: 072/075 | Batch 040/110 | Loss: 0.02\n",
      "Epoch: 072/075 | Batch 060/110 | Loss: 0.01\n",
      "Epoch: 072/075 | Batch 080/110 | Loss: 0.14\n",
      "Epoch: 072/075 | Batch 100/110 | Loss: 0.02\n",
      "Epoch: 073/075 | Batch 000/110 | Loss: 0.02\n",
      "Epoch: 073/075 | Batch 020/110 | Loss: 0.03\n",
      "Epoch: 073/075 | Batch 040/110 | Loss: 0.14\n",
      "Epoch: 073/075 | Batch 060/110 | Loss: 0.04\n",
      "Epoch: 073/075 | Batch 080/110 | Loss: 0.20\n",
      "Epoch: 073/075 | Batch 100/110 | Loss: 0.02\n",
      "Epoch: 074/075 | Batch 000/110 | Loss: 0.02\n",
      "Epoch: 074/075 | Batch 020/110 | Loss: 0.13\n",
      "Epoch: 074/075 | Batch 040/110 | Loss: 0.05\n",
      "Epoch: 074/075 | Batch 060/110 | Loss: 0.06\n",
      "Epoch: 074/075 | Batch 080/110 | Loss: 0.03\n",
      "Epoch: 074/075 | Batch 100/110 | Loss: 0.03\n",
      "Epoch: 075/075 | Batch 000/110 | Loss: 0.03\n",
      "Epoch: 075/075 | Batch 020/110 | Loss: 0.02\n",
      "Epoch: 075/075 | Batch 040/110 | Loss: 0.06\n",
      "Epoch: 075/075 | Batch 060/110 | Loss: 0.08\n",
      "Epoch: 075/075 | Batch 080/110 | Loss: 0.01\n",
      "Epoch: 075/075 | Batch 100/110 | Loss: 0.02\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "41396cca-8920-4edd-9075-588c03d81f01",
   "metadata": {},
   "source": [
    "## 7) Evaluating the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f458e2cc-011c-48e5-b66a-5ef568114242",
   "metadata": {},
   "source": [
    "Again, reusing the code from Unit 3.6, we will calculate the training and validation set accuracy.\n",
    "\n",
    "<font color='red'>Use the code below as is. What do you observe? And why?</font>"
   ]
  },
  {
   "cell_type": "code",
   "id": "4b6473aa-98ac-4ffe-84b5-cb5a2d511018",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:25.643718Z",
     "start_time": "2025-02-18T15:47:25.639809Z"
    }
   },
   "source": [
    "def compute_accuracy(model, dataloader):\n",
    "\n",
    "    model = model.eval()\n",
    "    \n",
    "    correct = 0.0\n",
    "    total_examples = 0\n",
    "    \n",
    "    for idx, (features, class_labels) in enumerate(dataloader):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            probas = model(features)\n",
    "        \n",
    "        pred = torch.where(probas > 0.5, 1, 0)\n",
    "        lab = class_labels.view(pred.shape).to(pred.dtype)\n",
    "\n",
    "        compare = lab == pred\n",
    "        correct += torch.sum(compare)\n",
    "        total_examples += len(compare)\n",
    "\n",
    "    return correct / total_examples"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "2c4e70f5-0fbd-4782-a7b0-75c687c5b6d9",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:25.688876Z",
     "start_time": "2025-02-18T15:47:25.663331Z"
    }
   },
   "source": [
    "train_acc = compute_accuracy(model, train_loader)\n",
    "print(f\"Accuracy: {train_acc*100:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.41%\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "c01e464b-cc38-41b7-9d7f-6baafba73f56",
   "metadata": {},
   "source": [
    "<font color='red'>Notice that the code validation accuracy is not shown? It's part of the exercise to implement it :)</font>"
   ]
  },
  {
   "cell_type": "code",
   "id": "7edead56-db64-4667-8007-937ab1974ac0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:25.808208Z",
     "start_time": "2025-02-18T15:47:25.795919Z"
    }
   },
   "source": [
    "## SOLUTION\n",
    "\n",
    "val_acc = compute_accuracy(model, val_loader)\n",
    "print(f\"Accuracy: {val_acc*100:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.45%\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "86d11c57-daf1-4d33-a609-99184c59752d",
   "metadata": {},
   "source": [
    "<font color='red'>Now, add the standardization to the `compute_accuracy` function above and recompute the training and validation accuracy. What do you observe?</font>"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:25.963901Z",
     "start_time": "2025-02-18T15:47:25.918890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_accuracy(model, dataloader):\n",
    "\n",
    "    model = model.eval()\n",
    "    \n",
    "    correct = 0.0\n",
    "    total_examples = 0\n",
    "    \n",
    "    for idx, (features, class_labels) in enumerate(dataloader):\n",
    "        \n",
    "        features = standardize(features, train_mean, train_std) ## SOLUTION\n",
    "        with torch.no_grad():\n",
    "            probas = model(features)\n",
    "        \n",
    "        pred = torch.where(probas > 0.5, 1, 0)\n",
    "        lab = class_labels.view(pred.shape).to(pred.dtype)\n",
    "\n",
    "        compare = lab == pred\n",
    "        correct += torch.sum(compare)\n",
    "        total_examples += len(compare)\n",
    "\n",
    "    return correct / total_examples\n",
    "\n",
    "train_acc = compute_accuracy(model, train_loader)\n",
    "print(f\"\\nAccuracy: {train_acc*100:.2f}%\")\n",
    "\n",
    "\n",
    "val_acc = compute_accuracy(model, val_loader)\n",
    "print(f\"Accuracy: {val_acc*100:.2f}%\")"
   ],
   "id": "d6c25ebf491d183d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 98.09%\n",
      "Accuracy: 98.18%\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "2f972979dfa5b1b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## OBSERVATIONS \n",
    "Since the loss of the course of the epoch steadily reduces and converges to 0, we can see that the standardization function is working correctly. We know that this model is strong in generalization because of the training and validation sets being so close in accuracy. This also shows that there is no overfitting going on with the model."
   ],
   "id": "fbec40ddff8567d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:26.089113Z",
     "start_time": "2025-02-18T15:47:26.086566Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d0db98ec473c9a8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:47:26.114867Z",
     "start_time": "2025-02-18T15:47:26.112550Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "806306420641925c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
